{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext iminizinc\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "# import ipywidgets as widgets\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from functools import *\n",
    "\n",
    "import pandas_ta as ta\n",
    "\n",
    "from datetime import datetime\n",
    "# import talib\n",
    "import pandas_ta as ta\n",
    "# from talib.abstract import *\n",
    "from math import *\n",
    "from collections import OrderedDict\n",
    "# import vectorbt as vbt    \n",
    "import json\n",
    "from decimal import *\n",
    "getcontext().prec = 6\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "import pickle\n",
    "from scipy.signal import argrelextrema,argrelmin, argrelmax\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from research.pkg.lib import *\n",
    "\n",
    "\n",
    "# import mplfinance as mpf\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['lines.linewidth'] = 1 \n",
    "\n",
    "mpl.rcParams.update({'font.size': 10, 'lines.linewidth': 1, 'figure.dpi': 100})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame so 'ta' can be used.\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Help about this, 'ta', extension\n",
    "# help(df.ta)\n",
    "df.ta.indicators()\n",
    "# df.a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 'BTC_USDT'\n",
    "timeframe = '1H'\n",
    "exchange = 'binance'\n",
    "\n",
    "# exchange = 'kucoin'\n",
    "# odf = pd.read_json(f'../../freq-user-data/data/{exchange}/futures/{pair}-{timeframe}-futures.json'\n",
    "\n",
    "def load_candles(exchange, pair, timeframe):\n",
    "    odf = pd.read_json(f'../freq-user-data/data/{exchange}/{pair}-{timeframe.lower()}.json'\n",
    "    ).dropna().set_axis(['timestamp', 'open', 'high', 'low', 'close', 'volume'], axis=1\n",
    "    # odf['dtime'] = pd.to_datetime(odf['timestamp'], unit='ms', utc=True).dt.tz_localize(None)\n",
    "    ).assign(dtime=lambda x: pd.to_datetime(x['timestamp'], unit='ms', utc=True).dt.tz_localize(None)\n",
    "    ).set_index('dtime').sort_index().drop('timestamp', axis=1)\n",
    "    return odf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All candle stick patterns under the sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "odf = load_candles(exchange, pair, timeframe)\n",
    "odf = odf.iloc[odf.shape[0] // 2:\n",
    "               ]\n",
    "lret = odf.close.divide(odf.open).apply(np.log)\n",
    "\n",
    "cdls = odf.ta.cdl_pattern(name=\"all\")\n",
    "cdls.shape, cdls.head(5)\n",
    "# odf.close.ewm(lag).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Averages and rolling standard deviations and z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "edf = pd.DataFrame({\n",
    "    k: v \n",
    "    \n",
    "    for lag in [6,12,21,50]\n",
    "    for edf in [odf.close.ewm(lag)]\n",
    "    for rdf in [odf.close.rolling(lag)]\n",
    "    for [emean,estd] in [[edf.mean(), edf.std()]]\n",
    "    for [rmean,rstd] in [[rdf.mean(), rdf.std()]]\n",
    "    for k, v in [\n",
    "        (f'ema_{lag}' , emean),\n",
    "        (f'estd_{lag}' , estd),\n",
    "        (f'eZscore', (odf.close - emean) / estd),\n",
    "        (f'rma_{lag}' , rmean),\n",
    "        (f'rstd_{lag}' , rstd),\n",
    "        (f'rZscore', (odf.close - rmean) / rstd)\n",
    "    ]\n",
    "\n",
    "})\n",
    "edf.shape, edf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past plen periods flattened to current row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_past_n(odf, nper=None):\n",
    "# plen = 6 * 2\n",
    "    # if columns is None: columns = odf.columns.tolist()\n",
    "    pcols = [f'{c}_m{nper-j}' for c in odf.columns for j in range(nper)]\n",
    "    fdf = pd.DataFrame([\n",
    "        (\n",
    "            odf.iloc[i-nper:i].values.flatten()\n",
    "        )\n",
    "        #.shape\n",
    "        for prange in [range(nper,odf.shape[0])]\n",
    "        for i in prange\n",
    "    ], index=odf.index.values[nper:]).set_axis(pcols, axis=1)\n",
    "\n",
    "    return fdf\n",
    "\n",
    "flatten_past_n(odf.iloc[:100][['high']], nper=4)#.index\n",
    "# xdf = odf.join(fdf)#.join(edf)\n",
    "\n",
    "# xdf[odf.columns.tolist() + [f'{c}_m{j}' for c in odf.columns for j in range(1,3) ]].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xdf.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher timeframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "H_periods = ['2H', '4H', '8H', '1D', '3D', '1W-Mon', '2W-Mon', '1M', 'Q', '1Y']\n",
    "H_odfs = {\n",
    "    timeframe: odf,\n",
    "    **{period: odf.resample(period, closed='left', label='left').agg(OrderedDict([\n",
    "        ('open', 'first'),\n",
    "        ('high', 'max'),\n",
    "        ('low', 'min'),\n",
    "        ('close', 'last'),\n",
    "        ('volume', 'sum'),\n",
    "    ])).set_axis([f'H_{period}_{c}' for c in odf.columns], axis=1)\n",
    "    for period in H_periods #[p.replace('-','_') for p in H_periods]\n",
    "    }\n",
    "}\n",
    "# hdf = reduce(lambda x,y: pd.merge_asof(x,y, direction='backward', left_index=True, right_index=True) ,\n",
    "#     [odf] + H_odfs)#.drop('dTime')\n",
    "# hdf['2023-05-01':].head()\n",
    "\n",
    "hdf = reduce(lambda x,y: pd.merge_asof(x,y, direction='backward', left_index=True, right_index=True) ,\n",
    "    H_odfs.values())\n",
    "hdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten N-number of past rows over higher time-frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_flats = [('2H', 12 * 3), ('4H', 6*7*1), ('8H', 3*7*4), ('1D', 7 * 4 ), ('3D', 10*3*3), ('1W-Mon', 4 * 4), ('2W-Mon', 2*2), ('1M', 3*0), ('Q', 4*1*0), ('1Y',0)]\n",
    "\n",
    "FH_dfs = {\n",
    "    timeframe: flatten_past_n(H_odfs[timeframe], nper=nper)\n",
    "    for timeframe, nper in h_flats if nper > 0\n",
    "}\n",
    "FH_dfs\n",
    "\n",
    "fhdf = reduce(lambda df1,df2: pd.merge_asof(df1,df2, direction='backward', left_index=True, right_index=True) ,\n",
    "    [FH_dfs[tf] for (tf, nper) in h_flats if nper > 0]\n",
    ")\n",
    "fhdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhdf.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_candles(wdf, ax=None, kwargs={}):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(**kwargs)\n",
    "\n",
    "    up, down = wdf[wdf.close >= wdf.open], wdf[wdf.close < wdf.open]\n",
    "    col1,col2 = 'green','red'\n",
    "    width, width2 = .8, .1\n",
    "    # Plotting up prices of the stock\n",
    "    ax.bar(up.index, up.close-up.open, width, bottom=up.open, color=col1)\n",
    "    ax.bar(up.index, up.high-up.close, width2, bottom=up.close, color=col1)\n",
    "    ax.bar(up.index, up.low-up.open, width2, bottom=up.open, color=col1)\n",
    "    # Plotting down prices of the stock\n",
    "    ax.bar(down.index, down.close-down.open, width, bottom=down.open, color=col2)\n",
    "    ax.bar(down.index, down.high-down.open, width2, bottom=down.open, color=col2)\n",
    "    ax.bar(down.index, down.low-down.close, width2, bottom=down.close, color=col2)    \n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    return ax\n",
    "\n",
    "def plot_H_candles(wdf, ax=None, columns=['open', 'high', 'low', 'close'], column_prefix='', column_suffix='', n_spans=1., alpha=0.3, align='edge', kwargs={}):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(**kwargs)\n",
    "        \n",
    "    copen, chigh, clow, cclose = [f'{column_prefix}{c}{column_suffix}' for c in columns]\n",
    "    up, down = wdf[wdf[cclose] >= wdf[copen]], wdf[wdf[cclose] < wdf[copen]]\n",
    "    col1,col2 = 'green','red'\n",
    "    width, width2 = n_spans * 1.,  .02 * n_spans\n",
    "\n",
    "    # # Plotting up prices of the stock\n",
    "    ax.bar(up.index.values, up[cclose]-up[copen], width, bottom=up[copen], color=col1, alpha=alpha, align=align)\n",
    "    ax.bar(up.index.values, up[chigh]-up[cclose], width, bottom=up[cclose], color=col1, alpha=alpha * 0.3, align=align)\n",
    "    ax.bar(up.index.values, up[clow]-up[copen], width, bottom=up[copen], color=col1, alpha=alpha* 0.3, align=align)\n",
    "    # # Plotting down prices of the stock\n",
    "    ax.bar(down.index.values, down[cclose]-down[copen], width, bottom=down[copen], color=col2, alpha=alpha, align=align)\n",
    "    ax.bar(down.index.values, down[chigh]-down[copen], width, bottom=down[copen], color=col2, alpha=alpha*0.3, align=align)\n",
    "    ax.bar(down.index.values, down[clow]-down[cclose], width, bottom=down[cclose], color=col2, alpha=alpha*0.3, align=align)   \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = hdf[-220:]\n",
    "ax = gdf.close.plot(figsize=(20,9), lw=0.5)\n",
    "\n",
    "ghdf = H_odfs['1D'].loc[gdf.index[0]:gdf.index[-1]]\n",
    "plot_candles(gdf, ax=ax)\n",
    "plot_H_candles(ghdf, ax=ax, column_prefix='H_1D_', n_spans=24 * 1, alpha=0.2)\n",
    "\n",
    "for h in ghdf.index.values: ax.axvline(h, lw=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal fully-connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fhdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xdf = odf.join(fhdf).dropna().iloc[-2048:,:]\n",
    "# wlen = 6 * 2\n",
    "\n",
    "# fdf = pd.DataFrame([\n",
    "#     (\n",
    "#         odf.iloc[i-wlen:i].values.flatten()\n",
    "#     )\n",
    "#     #.shape\n",
    "#     for i in range(wlen,odf.shape[0])\n",
    "# ], index=odf.index.values[wlen:])\n",
    "\n",
    "# xdf = odf.join(fdf).join(edf).dropna()\n",
    "\n",
    "# xdf.columns\n",
    "\n",
    "\n",
    "timeout = 6\n",
    "r2r = 2. / 1.\n",
    "target_pct = 0.02\n",
    "#################\n",
    "# fu_highs = dxdf.high.rolling(timeout).max().shift(-timeout)\n",
    "# fu_high_rets = fu_highs.divide(dxdf.close).apply(np.log).fillna(0).round(3)\n",
    "# fu_lows = dxdf.low.rolling(timeout).min().shift(-timeout)\n",
    "# fu_low_rets = fu_lows.divide(dxdf.close).apply(np.log).fillna(0).round(3)\n",
    "#################\n",
    "\n",
    "fu_highs = xdf.close.rolling(timeout).max().shift(-timeout)\n",
    "fu_high_rets = fu_highs.divide(xdf.close).apply(np.log).fillna(0).round(3)\n",
    "fu_lows = xdf.close.rolling(timeout).min().shift(-timeout)\n",
    "fu_low_rets = fu_lows.divide(xdf.close).apply(np.log).fillna(0).round(3)\n",
    "fu_r2r = - fu_low_rets / fu_high_rets\n",
    "# fu_rets\n",
    "# qs = np.quantile(fu_rets, np.linspace(0,1,6))\n",
    "# y_cut, y_bins = pd.qcut(fu_rets, np.linspace(0,1,8), retbins=True)\n",
    "# y = y_cut.map(lambda x: x.right).astype(float).values \n",
    "cats = pd.DataFrame(\n",
    "    (fu_high_rets >= target_pct)\n",
    "        # (fu_rets >= target_ret) & (fu_dds >= sl_ret)\n",
    "        # fu_lows < ema - stds * 3\n",
    "    & (fu_high_rets / (- fu_low_rets) > r2r)\n",
    "        # np.where( fu_r2r < 1/2.5 , 1, np.where(fu_r2r > 2.5, 1, 0))\n",
    "        # & (fu_low_rets > 0.01)\n",
    "        # fu_low_rets > -0.01\n",
    "     , index=xdf.index).dropna()#.astype(str)\n",
    "\n",
    "\n",
    "# wsel = cats.shape[0] // 2\n",
    "# sel_cats = cats.iloc[wsel:]\n",
    "sel_cats = cats\n",
    "y = sel_cats.values.ravel()\n",
    "Xdf = xdf.loc[sel_cats.index] #.values\n",
    "\n",
    "# iX_train, iX_test, y_train, y_test = train_test_split(i_samples, y, random_state=1, test_size=0.8)\n",
    "Xdf_train, Xdf_test, y_train, y_test = train_test_split(Xdf, y, random_state=7, shuffle=False, test_size=0.3)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7, test_size=0.7)\n",
    "X_train, X_test, = Xdf_train.values, Xdf_test.values\n",
    "\n",
    "clf = MLPClassifier(solver='adam', max_iter=200, verbose=False, batch_size=1024, \n",
    "                    activation='relu', epsilon=1e-10, hidden_layer_sizes=(128,128,128)\n",
    "                #     learning_rate_init = 1e-4,\n",
    "                    )\n",
    "\n",
    "scaler = StandardScaler().fit([*X_train, *X_test])\n",
    "\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "m = clf.fit(X_train_norm, y_train)\n",
    "\n",
    "y_pred_train =  m.predict(X_train_norm)\n",
    "y_pred_test =  m.predict(X_test_norm)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train,y_pred_train)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "# y_pred_proba = m.predict_proba(X_test)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "# auc_score = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "# fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba[:,1])\n",
    "\n",
    "# print('ROC curve (area = {:0.2f})'.format(auc_score))\n",
    "print(f'Acc(train)={accuracy_train:.2f}\\tAcc(test)={accuracy_train:.2f}')\n",
    "print(f'Precision(Train)={precision_train:.2f}\\tRecall(Train)={recall_train:.2f}')\n",
    "print(f'Precision(Test)={precision_test:.2f}\\tRecall(Test)={recall_test:.2f}')\n",
    "print(f'----------- Classification Report: ------------')\n",
    "report = classification_report(y_test, y_pred_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, Flatten, Activation, Conv3D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# xdf = hdf.dropna().iloc[-2048:,:]\n",
    "xdf = odf.join(fhdf).dropna().iloc[-2*2048:,:]\n",
    "\n",
    "timeout = 2\n",
    "# r2r = 2. / 1.\n",
    "# target_pct = 0.02\n",
    "\n",
    "# fu_highs = xdf.close.rolling(timeout).max().shift(-timeout)\n",
    "# fu_high_rets = fu_highs.divide(xdf.close).apply(np.log).fillna(0).round(3)\n",
    "# fu_lows = xdf.close.rolling(timeout).min().shift(-timeout)\n",
    "# fu_low_rets = fu_lows.divide(xdf.close).apply(np.log).fillna(0).round(3)\n",
    "# fu_r2r = - fu_low_rets / fu_high_rets\n",
    "# cats = pd.DataFrame(\n",
    "#     (fu_high_rets >= target_pct)\n",
    "#     & (fu_high_rets / (- fu_low_rets) > r2r)\n",
    "#      , index=xdf.index).dropna()\n",
    "\n",
    "# sel_cats = cats\n",
    "# y = sel_cats.values#.ravel()\n",
    "\n",
    "y = xdf.high.rolling(timeout).max().shift(-timeout).divide(xdf.close).apply(np.log).dropna().round(3)\n",
    "Xdf = xdf.loc[y.index] #.values\n",
    "\n",
    "\n",
    "X = Xdf.values\n",
    "y = y.values\n",
    "\n",
    "dataset = Xdf.values\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler.fit(Xdf.values)\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(Xdf) * 0.3)\n",
    "test_size = len(Xdf) - train_size\n",
    "# dfXtrain, dfXtest = Xdf.iloc[0:train_size,:], Xdf.iloc[train_size:len(Xdf),:]\n",
    "# dfYtrain, dfYtest = y.iloc[0:train_size,:], y.iloc[train_size:len(Xdf),:]\n",
    "\n",
    "Xtrain, Xtest = dataset[0:train_size], dataset[train_size:len(Xdf)]\n",
    "Ytrain, Ytest = y[0:train_size], y[train_size:len(Xdf)]\n",
    "\n",
    "\n",
    "train_X = Xtrain.reshape((Xtrain.shape[0], 1, Xtrain.shape[1]))\n",
    "test_X = Xtest.reshape((Xtest.shape[0], 1, Xtest.shape[1]))\n",
    "train_y = Ytrain.reshape(-1,1)\n",
    "test_y = Ytest.reshape(-1,1)\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "# print(len(Xtrain), len(Xtest))\n",
    "# train_X.shape, test_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.close.rolling(timeout).max().shift(-timeout).divide(xdf.close).apply(np.log).dropna().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# design network\n",
    "model = Sequential([\n",
    "    LSTM(4, input_shape=(train_X.shape[1], train_X.shape[2])),\n",
    "    Conv3D(filters=256, kernel_size=5, padding='same', activation='relu', input_shape=(1,X.shape[0])),\n",
    "    Flatten(),\n",
    "    Dense(64),\n",
    "    Activation('relu'),\n",
    "    Dense(32),\n",
    "    Activation('softmax'),\n",
    "    Dense(1),\n",
    "])\n",
    "model.add(LSTM(4, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "# model.add(Dense(15))\n",
    "# model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=150, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "import sklearn\n",
    "\n",
    "...\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "# test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# # invert scaling for forecast\n",
    "# inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "# inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "# inv_yhat = inv_yhat[:,0]\n",
    "# # invert scaling for actual\n",
    "# test_y = test_y.reshape((len(test_y), 1))\n",
    "# inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "# inv_y = scaler.inverse_transform(inv_y)\n",
    "# inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(test_y.reshape(-1), yhat.reshape(-1)))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "sklearn.metrics.mean_absolute_error(test_y, yhat)\n",
    "# print(f'----------- Classification Report: ------------')\n",
    "# report = classification_report(test_y.reshape(1,-1), yhat.reshape(1,-1))\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdf = pd.DataFrame([test_y.reshape(-1), yhat.reshape(-1)]).T\n",
    "# zdf.apply(lambda x: x[1] - x[0], axis=1).pow(2).sum() / yhat.shape[0]\n",
    "# zdf[zdf[1] > 0.01]\n",
    "# zdf[1].abs().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.reshape(-1), yhat.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming you have your OHLC data in a CSV file or any other source\n",
    "# df = pd.read_csv('ohlc_data.csv')  # Replace 'ohlc_data.csv' with the actual file path\n",
    "df = Xdf_test[['open', 'high', 'low', 'close']].assign(signal=y_pred_test)#.iloc[:200,:]#.copy()\n",
    "# signals = y_pred_test\n",
    "# Assuming you have an array of signals indicating when to go long (1) or not (0)\n",
    "# signals = np.array([0, 1, 1, 0, 1, 0, 0, 1, 0, 0])  # Replace this with your actual signals array\n",
    "\n",
    "take_profit_pct = 0.03  # 2% take-profit level\n",
    "stop_loss_pct = 0.009    # 1% stop-loss level\n",
    "\n",
    "entry_prices = df.close * (df.signal == 1)\n",
    "exit_prices = entry_prices * (1. + take_profit_pct)\n",
    "stop_prices = entry_prices * (1. - stop_loss_pct)\n",
    "a_close = df.close.values\n",
    "a_high = df.high.values\n",
    "a_low = df.low.values\n",
    "a_signal = df.signal.values\n",
    "\n",
    "\n",
    "\n",
    "returns = np.zeros(df.shape[0])\n",
    "# entry_price, exit_price, stop_price = None,None,None\n",
    "i_pos = None\n",
    "for i in range(len(df)):\n",
    "    if a_signal[i] == 1 and i_pos is None:  # Enter long position\n",
    "        i_pos = i\n",
    "        entry_price = entry_prices.iloc[i]; exit_price = exit_prices[i]; stop_price=stop_prices[i]\n",
    "\n",
    "    elif i_pos is not None:  # Check if take-profit or stop-loss is hit\n",
    "        # exit_price = df['close'].iloc[i]\n",
    "        if a_low[i] <= stop_prices[i_pos] or a_high[i] >= exit_prices[i_pos]:\n",
    "            if a_low[i] <= stop_prices[i_pos]:\n",
    "                exit_price = stop_prices[i_pos]  # Adjust exit price to take-profit\n",
    "            elif a_high[i] >= exit_prices[i_pos] :\n",
    "                exit_price =  exit_prices[i_pos]  # Adjust exit price to stop-loss\n",
    "            returns[i] = log(exit_price / entry_price) \n",
    "            # entry_price, exit_price, stop_price = None,None,None\n",
    "            i_pos = None\n",
    "\n",
    "# Check if there's an open position at the end of the data\n",
    "# if current_position == 1:\n",
    "#     exit_price = df['close'].iloc[-1]\n",
    "#     if exit_price >= entry_price * (1 + take_profit_pct):\n",
    "#         exit_price = entry_price * (1 + take_profit_pct)  # Adjust exit price to take-profit\n",
    "#     else:\n",
    "#         exit_price = entry_price * (1 - stop_loss_pct)  # Adjust exit price to stop-loss\n",
    "#     returns.append((exit_price / entry_price) - 1)\n",
    "\n",
    "df['pnl'] = returns\n",
    "df['cum_ret'] = df.pnl.fillna(0).cumsum()\n",
    "\n",
    "tdf = pd.DataFrame(index=xdf.index).join(df.pnl).fillna(0)\n",
    "tdf['cum_ret'] = tdf.pnl.cumsum()\n",
    "\n",
    "# plt.plot()\n",
    "tdf['cum_ret'].plot(figsize=(20,5))\n",
    "plt.xlabel('Trades')\n",
    "plt.ylabel('Cumulative Returns')\n",
    "plt.title('Cumulative Returns of Trading Strategy')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.pnl ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df.pnl < 0).value_counts()\n",
    "(df.pnl == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xpdf = Xdf_test[['open', 'high', 'low', 'close']].assign(signal=y_pred_test)\n",
    "\n",
    "# da_printer(xpdf, 'nn-kitchen-sink', strategy_params = {})\n",
    "strategy_params = {}\n",
    "strategy =  'nn-kitchen-sink'\n",
    "strategy_params_json = f'./par-{strategy}.json'\n",
    "    ### windowing\n",
    "olen = xpdf.shape[0]\n",
    "w2log = 7\n",
    "wlen = 2**w2log\n",
    "w2len = wlen // 2\n",
    "nwin = olen // w2len\n",
    "# olen, w2log, wlen, w2len, nwin\n",
    "###\n",
    "fu_params = {\n",
    "        'w2log':    {'wdg': IntSlider(description=\"n2\", min=0, max=13, step=1, value=w2log)}, \n",
    "        'w':        {'wdg': IntSlider(description=\"w\", min=0, max=nwin, step=1, value=1)},\n",
    "        \"candles\":  {\"wdg\": Checkbox(value=True, description='Candles', disabled=False)},\n",
    "    }\n",
    "\n",
    "def update_sl_w_range(kwargs):\n",
    "    print('*kwargs', kwargs)\n",
    "    wlen = 2**kwargs['new'] #fu_params['w2loog'].value\n",
    "\n",
    "    w2len = wlen // 2\n",
    "    nwin = olen // w2len\n",
    "    fu_params['w']['wdg'].value = 0\n",
    "    fu_params['w']['wdg'].max = nwin\n",
    "fu_params['w2log']['wdg'].observe(update_sl_w_range, 'value')\n",
    "\n",
    "all_params = {\n",
    "    ** fu_params,\n",
    "    **strategy_params\n",
    "}\n",
    "wdgts = [pv['wdg'] for pk, pv in all_params.items()]\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    widgets.VBox([widgets.HBox(wdgts[i:i+4]) for i in range(0, len(wdgts), 4)])\n",
    "])\n",
    "\n",
    "if os.path.exists(strategy_params_json):\n",
    "    with open(strategy_params_json) as f: \n",
    "        js = json.loads(f.read());\n",
    "        for k, v in all_params.items(): \n",
    "            if k in js: v['wdg'].value=js[k];\n",
    "\n",
    "else: print(f'File not found: {strategy_params_json}')\n",
    "\n",
    "wdf=None\n",
    "def printer(**pkwargs):   \n",
    "    w2log, w, candles = pkwargs['w2log'], pkwargs['w'], pkwargs['candles'],\n",
    "\n",
    "    wlen = 2**w2log\n",
    "    w2len = wlen // 2\n",
    "    nwin = olen // w2len\n",
    "\n",
    "    with open(strategy_params_json, \"w\") as f: f.write(json.dumps({k: v['wdg'].value for k, v in all_params.items()}))\n",
    "    \n",
    "    \n",
    "    wst = w * w2len\n",
    "    wed = min(olen-1, wst + wlen)\n",
    "    wlen = wed - wst + 1\n",
    "    if olen - wed + 1 < w2len: wed = - 1\n",
    "    # print(f'wst:{wst}, wed:{wed}')\n",
    "    wdf = xpdf.iloc[wst:wed,:].copy()    \n",
    "\n",
    "\n",
    "    wsignals = wdf.loc[wdf.signal].index\n",
    "\n",
    "    print(f'N={len(wdf)}; Period: {wdf.index[0] - wdf.index[-1]}, Start: {wdf.index[0]}, End: {wdf.index[-1]}\\n')\n",
    "    # print('plt.rcParam[\"lines.linewidth\"]: ', plt.rcParams[\"lines.linewidth\"])\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(20,6))\n",
    "    ax_dict = fig.subplot_mosaic(\"\"\"\n",
    "        AAA\n",
    "        BBB\n",
    "        XYZ\n",
    "        \"\"\",\n",
    "        height_ratios=[3, 1, 1],\n",
    "    )\n",
    "    identify_axes(ax_dict)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    axa = ax_dict['A']\n",
    "    axb = ax_dict['B']\n",
    "    wdf.close.multiply(1.1).plot(ax=axa, lw=0.4, alpha=0.);\n",
    "\n",
    "\n",
    "    if candles: plot_candles(wdf, ax=axa)\n",
    "    else: wdf.close.plot(ax=axa, c='b');\n",
    "    for ix in wsignals: axa.axvline(ix, c='g')\n",
    "\n",
    "    # if draw_func is not None: draw_func(ax_dict, indicators, signals, pkwargs)\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "out = widgets.interactive_output(printer, {\n",
    "        **{k : v['wdg'] for k,v in all_params.items()}\n",
    "    });\n",
    "x = display(ui, out);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sel_cats = cats.iloc[:wsel]\n",
    "test_cats = cats.iloc[wsel:]\n",
    "y_test = test_cats.values\n",
    "X_test = xdf.loc[test_cats.index].values\n",
    "\n",
    "# scaler = StandardScaler().fit(X_test)\n",
    "\n",
    "# X_train_norm = StandardScaler().fit(X_test).transform(X_train)\n",
    "\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "y_pred_test =  m.predict(X_test_norm)\n",
    "\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "print(f'Precision(Test)={precision_test:.2f}\\tRecall(Test)={recall_test:.2f}')\n",
    "print(f'----------- Classification Report: ------------')\n",
    "report = classification_report(y_test, y_pred_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print(i):\n",
    "    j = iX_test[i]\n",
    "    wdf = odf.iloc[j - wlen:j + timeout].copy()\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax1 = fig.subplots(1,1)\n",
    "    ax1.axvline(odf.index[j])\n",
    "    # plot_candles(wdf,ax=ax1)\n",
    "    wdf.close.plot(ax=ax1, color='red' if y_pred_test[i] else 'gray')\n",
    "\n",
    "interactive(print, i=(0,iX_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256 * 128 *128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7 * 7 * 128 * 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
