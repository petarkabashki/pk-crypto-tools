{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "# import pytz\n",
    "import json\n",
    "import numpy as np \n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "from pprint import pprint\n",
    "\n",
    "import schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_fetch_ohlcv(exchange_id, max_retries, symbol, timeframe, since, limit):\n",
    "    exchange = getattr(ccxt, exchange_id)({\n",
    "        'enableRateLimit': True,  # required by the Manual\n",
    "    })\n",
    "    num_retries = 0\n",
    "    try:\n",
    "        num_retries += 1\n",
    "        ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since, limit)\n",
    "        # print('Fetched', len(ohlcv), symbol, 'candles from', exchange.iso8601 (ohlcv[0][0]), 'to', exchange.iso8601 (ohlcv[-1][0]))\n",
    "        return ohlcv\n",
    "    except Exception:\n",
    "        if num_retries > max_retries:\n",
    "            raise  # Exception('Failed to fetch', timeframe, symbol, 'OHLCV in', max_retries, 'attempts')\n",
    "\n",
    "\n",
    "def scrape_ohlcv(exchange_id, max_retries, symbol, timeframe, since, limit):\n",
    "    exchange = getattr(ccxt, exchange_id)({\n",
    "        'enableRateLimit': True,  # required by the Manual\n",
    "    })\n",
    "    timeframe_duration_in_seconds = exchange.parse_timeframe(timeframe)\n",
    "    timeframe_duration_in_ms = timeframe_duration_in_seconds * 1000\n",
    "    timedelta = limit * timeframe_duration_in_ms\n",
    "    now = exchange.milliseconds()\n",
    "    all_ohlcv = []\n",
    "    fetch_since = since\n",
    "    while fetch_since < now:\n",
    "        ohlcv = retry_fetch_ohlcv(exchange_id, max_retries, symbol, timeframe, fetch_since, limit)\n",
    "        fetch_since = (ohlcv[-1][0] + 1) if len(ohlcv) else (fetch_since + timedelta)\n",
    "        all_ohlcv = all_ohlcv + ohlcv\n",
    "        if len(all_ohlcv):\n",
    "            print(len(all_ohlcv), 'candles in total from', exchange.iso8601(all_ohlcv[0][0]), 'to', exchange.iso8601(all_ohlcv[-1][0]))\n",
    "        else:\n",
    "            print(len(all_ohlcv), 'candles in total from', exchange.iso8601(fetch_since))\n",
    "    return exchange.filter_by_since_limit(all_ohlcv, since, None, key=0)\n",
    "\n",
    "def scrape_candles_to_csv(filename, exchange_id, max_retries, symbol, timeframe, since, limit):\n",
    "    # instantiate the exchange by id\n",
    "    exchange = getattr(ccxt, exchange_id)({\n",
    "        'enableRateLimit': True,  # required by the Manual\n",
    "    })\n",
    "    # convert since from string to milliseconds integer if needed\n",
    "    if isinstance(since, str):\n",
    "        since = exchange.parse8601(since)\n",
    "    # preload all markets from the exchange\n",
    "    exchange.load_markets()\n",
    "    # fetch all candles\n",
    "    ohlcv = scrape_ohlcv(exchange, max_retries, symbol, timeframe, since, limit)\n",
    "    # save them to csv file\n",
    "    write_to_csv(filename, ohlcv)\n",
    "    print('Saved', len(ohlcv), f'candles for {exchange_id}, {symbol}, {timeframe} from', exchange.iso8601(ohlcv[0][0]), 'to', exchange.iso8601(ohlcv[-1][0]), 'to', filename)\n",
    "\n",
    "\n",
    "def scrape_candles_to_db(exchange_id, symbol, timeframe, since, to=None, max_retries=3, limit=100):\n",
    "    # instantiate the exchange by id\n",
    "    exchange = getattr(ccxt, exchange_id)({\n",
    "        'enableRateLimit': True,  # required by the Manual\n",
    "    })\n",
    "    # convert since from string to milliseconds integer if needed\n",
    "    if isinstance(since, str):\n",
    "        since = exchange.parse8601(since)\n",
    "    # preload all markets from the exchange\n",
    "    exchange.load_markets()\n",
    "    # fetch all candles\n",
    "    ohlcv = scrape_ohlcv(exchange_id, max_retries, symbol, timeframe, since, limit)\n",
    "    # ohlcv = ohlcv[0:-1]\n",
    "    if len(ohlcv) > 0:\n",
    "        df = pd.DataFrame(ohlcv)\n",
    "        df.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "        df['date'] = pd.to_datetime(df['timestamp'], unit='ms', utc=False)\n",
    "        df = df[['timestamp', 'date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "        if not to is None:\n",
    "            df = df[df.timestamp <= to]\n",
    "\n",
    "        db = mongo_client[exchange_id]\n",
    "        collection = db[f'{symbol}-{timeframe}']\n",
    "        collection.delete_many({'timestamp': {'$gte': since}})\n",
    "        collection.insert_many(df.to_dict(\"records\"))\n",
    "\n",
    "    print(f'Saved to DB {len(ohlcv)} candles for {exchange_id}, {symbol}, {timeframe}, {exchange.iso8601(ohlcv[0][0])} to {exchange.iso8601(ohlcv[-1][0])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tframe2msec = {\n",
    "  '1m': 1000 * 60 * 1,\n",
    "  '5m': 1000 * 60 * 5,\n",
    "  '15m': 1000 * 60 * 15,\n",
    "  '30m': 1000 * 60 * 30,\n",
    "  '1h': 1000 * 60 * 60,\n",
    "  '4h': 1000 * 60 * 60 * 4,\n",
    "  '8h': 1000 * 60 * 60 * 8,\n",
    "  '1d': 1000 * 60 * 60 * 24,\n",
    "}\n",
    "\n",
    "def get_now_btimestamp(timeframe):\n",
    "  delta = tframe2msec[timeframe]\n",
    "  now_stamp = int(datetime.now().timestamp() * 1000)\n",
    "  now_bstamp = divmod(now_stamp, delta)[0] * delta\n",
    "  return now_bstamp\n",
    "\n",
    "\n",
    "def get_latest_to_db(exchange_id, symbol, timeframe):\n",
    "  db = mongo_client[exchange_id]\n",
    "  collection_name = f'{symbol}-{timeframe}'\n",
    "  delta = tframe2msec[timeframe]\n",
    "  now_bstamp = get_now_btimestamp(timeframe) \n",
    "  collection = db[collection_name]\n",
    "  res = list(collection.find({'timestamp': {'$lt' : now_bstamp}}).sort([('timestamp', -1)]))\n",
    "  since = now_bstamp\n",
    "  if (len(res) == 0):\n",
    "    since = now_bstamp - 5 * delta\n",
    "  elif (res[0]['timestamp'] < now_bstamp - delta):\n",
    "    since = res[0]['timestamp'] - delta\n",
    "\n",
    "  if (since < now_bstamp):\n",
    "    print('Scraping to db:', exchange_id, symbol, timeframe)\n",
    "    scrape_candles_to_db(exchange_id, symbol, timeframe, since, to=now_bstamp-delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exchange_id = 'kucoinfutures'\n",
    "symbol = 'ADAUSDTM'\n",
    "timeframe = '5m'\n",
    "\n",
    "get_latest_to_db(exchange_id, symbol, timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# update latest candles from exchange\n",
    "exchange_ids = ['kucoinfutures']\n",
    "symbols = ['XBTUSDTM', 'DOTUSDTM', 'ADAUSDTM', 'ALGOUSDTM', 'SOLUSDTM']\n",
    "timeframes = ['5m', '1h', '8h', '1d']\n",
    "\n",
    "def check_N_update():\n",
    "  for exchange_id in exchange_ids:\n",
    "    for symbol in symbols:\n",
    "      for timeframe in timeframes:\n",
    "        get_latest_to_db(exchange_id, symbol, timeframe)\n",
    "\n",
    "schedule.every(5).seconds.do(check_N_update)\n",
    "    \n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "dyn_updater_module_name = 'kline_dyn_updater'\n",
    "updmodule = imp.new_module(dyn_updater_module_name)\n",
    "# updmodule\n",
    "sys.modules[dyn_updater_module_name] = updmodule\n",
    "# sys.modules.setdefault('kline_dyn_updater', updmodule)\n",
    "\n",
    "import kline_dyn_updater\n",
    "\n",
    "kline_dyn_updater\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger('apscheduler').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "from pytz import utc\n",
    "\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from apscheduler.jobstores.mongodb import MongoDBJobStore\n",
    "from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\n",
    "from apscheduler.executors.pool import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "\n",
    "jobstores = {\n",
    "    'mongo': MongoDBJobStore(),\n",
    "    'default': SQLAlchemyJobStore(url='sqlite:///jobs.sqlite')\n",
    "}\n",
    "executors = {\n",
    "    'default': ThreadPoolExecutor(50),\n",
    "    # 'processpool': ProcessPoolExecutor(5)\n",
    "}\n",
    "job_defaults = {\n",
    "    'coalesce': False,\n",
    "    'max_instances': 5\n",
    "}\n",
    "\n",
    "\n",
    "def get_latest_to_db_factory(exchange_id, symbol, timeframe):\n",
    "  def f():\n",
    "    get_latest_to_db(exchange_id, symbol, timeframe)\n",
    "  return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exchange_ids = ['kucoinfutures']\n",
    "symbols = ['XBTUSDTM', 'ETHUSDTM', 'DOTUSDTM', 'ADAUSDTM', 'ALGOUSDTM', 'SOLUSDTM']\n",
    "timeframes = ['1m', '5m', '15m', '1h', '4h', '8h', '1d']\n",
    "\n",
    "\n",
    "logging.getLogger('apscheduler').setLevel(logging.ERROR)\n",
    "\n",
    "apscheduler = BackgroundScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults, timezone=utc)\n",
    "\n",
    "for exchange_id in exchange_ids:\n",
    "  for symbol in symbols:\n",
    "    for timeframe in timeframes:\n",
    "      method_name = f'{exchange_id}_{symbol}_{timeframe}'\n",
    "      # print(f'->{method_name}')\n",
    "      setattr(kline_dyn_updater, method_name, get_latest_to_db_factory(exchange_id, symbol, timeframe))\n",
    "      apscheduler.add_job(f'kline_dyn_updater:{method_name}', 'interval', seconds=10, id=method_name)\n",
    "      \n",
    "\n",
    "if os.path.exists(\"jobs.sqlite\"): os.remove(\"jobs.sqlite\")\n",
    "apscheduler.start()\n",
    "# scheduler.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.getLogger('apscheduler').setLevel(logging.INFO)\n",
    "apscheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apscheduler.shutdown()\n",
    "# apscheduler.shutdown(wait=False)\n",
    "\n",
    "apscheduler.remove_all_jobs()\n",
    "if os.path.exists(\"jobs.sqlite\"): os.remove(\"jobs.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apscheduler.remove_all_jobs()\n",
    "# apscheduler.get_jobs()\n",
    "# for job in apscheduler.get_jobs(): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a7c4e175efa44dd81f4fbacfe90b94f14c2cef036973cca7ce5c2420ad37c9d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
